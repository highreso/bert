{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 13161667970752824330,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 39395347712\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 9371429395055466727\n",
       " physical_device_desc: \"device: 0, name: A100-PCIE-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check GPU recognized\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas==1.1.5\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 5.8 MB/s eta 0:00:01     |██▋                             | 788 kB 3.6 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from pandas==1.1.5) (1.19.2)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from pandas==1.1.5) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.5 pytz-2022.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_hub==0.12.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from tensorflow_hub==0.12.0) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from tensorflow_hub==0.12.0) (1.19.2)\n",
      "Requirement already satisfied: six>=1.9 in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow_hub==0.12.0) (1.15.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "\u001b[33m  WARNING: The scripts make_image_classifier and make_nearest_neighbour_index are installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tensorflow-hub-0.12.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bert-tensorflow==1.0.1\n",
      "  Downloading bert_tensorflow-1.0.1-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/anaconda3/envs/tensorflow24_py36/lib/python3.6/site-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pandas==1.1.5\n",
    "!pip install -U tensorflow_hub==0.12.0\n",
    "!pip install -U bert-tensorflow==1.0.1\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/user/.local/lib/python3.6/site-packages')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train =pd.read_csv(\"./input/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test =pd.read_csv(\"./input/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.91 s, sys: 1.68 s, total: 11.6 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile = tf.io.gfile\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after tokenization: \n",
      "['this', 'is', 'a', 'goat', ',', 'and', 'i', 'am', 'riding', 'a', 'boat', '.', '.', '.', '.']\n",
      "After adding [CLS] and [SEP]: \n",
      "['[CLS]', 'this', 'is', 'a', 'goat', ',', 'and', 'i', 'am', 'riding', 'a', 'boat', '.', '.', '.', '.', '[SEP]']\n",
      "After converting Tokens to Id: \n",
      "[101, 2023, 2003, 1037, 13555, 1010, 1998, 1045, 2572, 5559, 1037, 4049, 1012, 1012, 1012, 1012, 102]\n",
      "tokens: \n",
      "[101, 2023, 2003, 1037, 13555, 1010, 1998, 1045, 2572, 5559, 1037, 4049, 1012, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Pad Masking: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segment Ids: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a Goat, and I am riding a Boat....\"\n",
    "tokenize_ = tokenizer.tokenize(text)\n",
    "print(\"Text after tokenization: \")\n",
    "print(tokenize_)\n",
    "max_len = 25\n",
    "text = tokenize_[:max_len-2]\n",
    "input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "pad_len = max_len - len(input_sequence)\n",
    "print(\"After adding [CLS] and [SEP]: \")\n",
    "print(input_sequence)\n",
    "tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "print(\"After converting Tokens to Id: \")\n",
    "print(tokens)\n",
    "tokens += [0] * pad_len\n",
    "print(\"tokens: \")\n",
    "print(tokens)\n",
    "pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "print(\"Pad Masking: \")\n",
    "print(pad_masks)\n",
    "segment_ids = [0] * max_len\n",
    "print(\"Segment Ids: \")\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_Process_data(documents, tokenizer, max_len=512):\n",
    "    '''\n",
    "    For preprocessing we have regularized, transformed each upper case into lower case, tokenized,\n",
    "    Normalized and remove stopwords. For normalization, we have used PorterStemmer. Porter stemmer transforms \n",
    "    a sentence from this \"love loving loved\" to this \"love love love\"\n",
    "    \n",
    "    '''\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    print(\"Pre-Processing the Data.........\\n\")\n",
    "    for data in documents:\n",
    "        review = re.sub('[^a-zA-Z]', ' ', data)\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        review = url.sub(r'',review)\n",
    "        html=re.compile(r'<.*?>')\n",
    "        review = html.sub(r'',review)\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        review = emoji_pattern.sub(r'',review)\n",
    "        text = tokenizer.tokenize(review)\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_id (InputLayer)         [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           keras_layer[0][1]                \n",
      "==================================================================================================\n",
      "Total params: 109,482,241\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "shape of _ layer of BERT: (None, 768)\n",
      "shape of last layer of BERT: (None, None, 768)\n"
     ]
    }
   ],
   "source": [
    "input_word_id = Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_id = Input(shape=(max_len,), dtype=tf.int32, name = \"segment_id\")\n",
    "\n",
    "_, sequence_output = bert_layer([input_word_id, input_mask, segment_id])\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "model = Model(inputs=[input_word_id, input_mask, segment_id],outputs=clf_output)\n",
    "model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(\"shape of _ layer of BERT: \"+str(_.shape))\n",
    "print(\"shape of last layer of BERT: \"+str(sequence_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_id = Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_id = Input(shape=(max_len,), dtype=tf.int32, name = \"segment_id\")\n",
    "    \n",
    "    _, sequence_output = bert_layer([input_word_id, input_mask, segment_id])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    dense_layer1 = Dense(units=256,activation='relu')(clf_output)\n",
    "    dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "    dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
    "    dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "    out = Dense(1, activation='sigmoid')(dense_layer2)\n",
    "    \n",
    "    model = Model(inputs=[input_word_id, input_mask, segment_id],outputs=out)\n",
    "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing the Data.........\n",
      "\n",
      "Pre-Processing the Data.........\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input = pre_Process_data(train.text.values, tokenizer, max_len=260)\n",
    "test_input = pre_Process_data(test.text.values, tokenizer, max_len=260)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 260)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 260)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_id (InputLayer)         [(None, 260)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 768)          0           keras_layer[1][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,712,130\n",
      "Trainable params: 109,712,129\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=260)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 57s 214ms/step - loss: 0.5645 - accuracy: 0.7117 - val_loss: 0.3881 - val_accuracy: 0.8286\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 39s 206ms/step - loss: 0.3563 - accuracy: 0.8603 - val_loss: 0.3911 - val_accuracy: 0.8240\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 40s 208ms/step - loss: 0.2281 - accuracy: 0.9180 - val_loss: 0.4456 - val_accuracy: 0.8240\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 42s 219ms/step - loss: 0.1243 - accuracy: 0.9601 - val_loss: 0.5872 - val_accuracy: 0.8168\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 46s 243ms/step - loss: 0.0767 - accuracy: 0.9760 - val_loss: 0.7556 - val_accuracy: 0.8221\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 47s 247ms/step - loss: 0.0585 - accuracy: 0.9764 - val_loss: 1.0744 - val_accuracy: 0.8175\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 49s 256ms/step - loss: 0.0444 - accuracy: 0.9821 - val_loss: 0.7293 - val_accuracy: 0.8004\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 48s 250ms/step - loss: 0.0502 - accuracy: 0.9819 - val_loss: 1.0296 - val_accuracy: 0.8024\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 49s 255ms/step - loss: 0.0448 - accuracy: 0.9827 - val_loss: 1.0033 - val_accuracy: 0.8142\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 50s 264ms/step - loss: 0.0260 - accuracy: 0.9906 - val_loss: 0.9836 - val_accuracy: 0.8096\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=32\n",
    "    # batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"./input/sample_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83996207],\n",
       "       [0.7478612 ],\n",
       "       [0.8283376 ],\n",
       "       ...,\n",
       "       [0.97025764],\n",
       "       [0.8376532 ],\n",
       "       [0.5554967 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow24_py36)",
   "language": "python",
   "name": "conda_tensorflow24_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
